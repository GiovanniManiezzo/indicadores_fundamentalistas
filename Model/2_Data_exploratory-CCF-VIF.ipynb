{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from scipy.stats import norm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Opção 1: ignorar todos os warnings (global)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Opção 2: ignorar apenas categorias específicas (recomendado)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Pandas: desativar aviso de SettingWithCopy (use com cuidado)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# NumPy: ignorar avisos numéricos (overflow/underflow etc.)\n",
    "np.seterr(all=\"ignore\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando os dados\n",
    "\n",
    "Dados previamente tratados.\n",
    "Foram importados 48 índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "path_dados_tratados = os.getenv('PATH_DADOS_TRATADOS')\n",
    "\n",
    "dados_indicadores = pd.read_csv(path_dados_tratados + \"dados_indicadores_tratados.csv\")\n",
    "dados_indices = pd.read_csv(path_dados_tratados + \"dados_indices.csv\", sep=';')\n",
    "dados_indices.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "dados_indices.drop(columns=['incc'], inplace=True)\n",
    "preco_acoes = pd.read_csv(path_dados_tratados + 'preco_acoes.csv')\n",
    "preco_acoes = preco_acoes.iloc[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_indices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renomear colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {\n",
    "    \"IPAOG-DI Ipa - todos os itens - Nro. Índice(1477007)\": \"IPAOG-DI\",\n",
    "    \"INCC-Brasil-DI-Todos os itens(1464783)\": \"INCC-Brasil-DI\",\n",
    "    \"Indicador de Incerteza da Economia Brasil (IIE-Br)(1428452)\": \"IIE-Br\",\n",
    "    \"Indicador de Incerteza da Economia Brasil - Componente Mídia (Mídia - IIE-Br)(1463188)\": \"Mídia - IIE-Br\",\n",
    "    \"Indicador de Incerteza da Economia Brasil - Componente Expactativas (Expec IIE-Br)(1463189)\": \"Expec IIE-Br\",\n",
    "    \"Indicador Antecedente de Emprego (IA Emp) com ajuste sazonal(1416205)\": \"IA Emp\",\n",
    "    \"Indicador Antecedente de Emprego (IA Emp) com ajuste sazonal(1416205) (Variação Percentual em Relação ao Período Anterior)\": \"Var. IA Emp\",\n",
    "    \"PIB a preços de 1995(1428699)\": \"PIB 1995\",\n",
    "    \"PIB a preços correntes(1428698)\": \"PIB Corrente\",\n",
    "    \"Índice de volume mensal  - PIB a preços de mercado (1428676)\": \"Vol. PIB Mensal\",\n",
    "    \"Índice de volume mensal pela ótica da produção - Valor adicionado a preços básicos(1428677)\": \"Vol. VA Produção\",\n",
    "    \"Índice de volume mensal pela ótica da produção - Valor adicionado a preços básicos - Agropecuária(1428678)\": \"Vol. VA Agro\",\n",
    "    \"Índice de volume mensal pela ótica da produção - Valor adicionado a preços básicos - Indústria(1428679)\": \"Vol. VA Indústria\",\n",
    "    \"Índice de volume mensal pela ótica da produção - Valor adicionado a preços básicos - Serviços(1428680)\": \"Vol. VA Serviços\",\n",
    "    \"Índice de volume mensal pela ótica da produção - Impostos (1428681)\": \"Vol. Impostos\",\n",
    "    \"Índice de volume mensal com ajuste sazonal - PIB a preços de mercado (1428682)\": \"Vol. PIB Sazonal\",\n",
    "    \"Índice de volume mensal pela ótica da produção com ajuste sazonal -Valor adicionado (1428683)\": \"Vol. VA Sazonal\",\n",
    "    \"Índice de volume mensal pela ótica da produção com ajuste sazonal - Valor adicionado - Agropecuária(1428684)\": \"Vol. VA Agro Sazonal\",\n",
    "    \"Índice de volume mensal pela ótica da produção com ajuste sazonal - Valor adicionado - Indústria(1428685)\": \"Vol. VA Ind. Sazonal\",\n",
    "    \"Índice de volume mensal pela ótica da produção com ajuste sazonal - Valor adicionado - Serviços(1428686)\": \"Vol. VA Serv. Sazonal\",\n",
    "    \"Índice de volume trimestral com ajuste sazonal - PIB(1428700)\": \"Vol. PIB Trim. Saz.\",\n",
    "    \"Índice de volume trimestral com ajuste sazonal - Valor Adicionado(1428701)\": \"Vol. VA Trim. Saz.\",\n",
    "    \"Índice de volume trimestral com ajuste sazonal - Valor Adicionado - Agropecuária(1428702)\": \"Vol. VA Agro Trim. Saz.\",\n",
    "    \"Índice de volume trimestral com ajuste sazonal - Valor Adicionado - Indústria(1428703)\": \"Vol. VA Ind. Trim. Saz.\",\n",
    "    \"Índice de volume trimestral com ajuste sazonal - Valor Adicionado - Serviços(1428704)\": \"Vol. VA Serv. Trim. Saz.\",\n",
    "    \"Índice da Taxa de Câmbio Real - (Real / Dólar (EUA))(1003592)\": \"Câmbio Real/Dólar\",\n",
    "    \"Índice da Taxa de Câmbio Real - (Real / Iene (Japão))(1003593)\": \"Câmbio Real/Iene\",\n",
    "    \"Índice da Taxa de Câmbio Real - (Real / Peso (Argentina))(1003594)\": \"Câmbio Real/Peso\",\n",
    "    \"Índice da Taxa de Câmbio Real - (Real / Euro (Zona do Euro))(1003595)\": \"Câmbio Real/Euro\",\n",
    "    \"Índice da Taxa de Câmbio Real - (Real / Libra (Reino Unido))(1003596)\": \"Câmbio Real/Libra\",\n",
    "    \"Índice da Taxa de Câmbio Efetiva Real - (Real / Cesta de Moedas)(1003597)\": \"Câmbio Efetiva Real\",\n",
    "    \"Juros EUA\": \"Juros EUA\",\n",
    "    \"Tbonds\": \"T-bonds\",\n",
    "    \"Cotacao dolar\": \"Cotação Dólar\",\n",
    "    \"CDI\": \"CDI\",\n",
    "    \"TJLP\": \"TJLP\",\n",
    "    \"Energia res\": \"Energia Res.\",\n",
    "    \"Energia ind\": \"Energia Ind.\",\n",
    "    \"Energia comer\": \"Energia Com.\",\n",
    "    \"Divida/PIB\": \"Dívida/PIB\",\n",
    "}\n",
    "\n",
    "if 'dados_indices' in globals():\n",
    "    dados_indices.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Opcional: verificar quais colunas foram renomeadas em cada DF\n",
    "for name, df in [('dados_indicadores', globals().get('dados_indicadores')),\n",
    "                 ('dados_indices', globals().get('dados_indices'))]:\n",
    "    if df is not None:\n",
    "        renomeadas = [v for k, v in rename_map.items() if k in df.columns]\n",
    "        print(f\"{name}: {len(renomeadas)} colunas renomeadas.\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_indices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o foco deste estudo são as companhias de transmissão de energia elétrica, as seguintes empresas serão consideradas:\n",
    "* ELET6 - CD_CVM = 2437\n",
    "* ISAE4 - CD_CVM = 18376\n",
    "* TAEE4 - CD_CVM = 20257\n",
    "* ALUP4 - CD_CVM = 21490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar dataframe para possuir apenas os códigos 2437, 18376, 20257, 21490\n",
    "dados_indicadores_transm = dados_indicadores[dados_indicadores['CD_CVM'].isin([2437, 18376, 20257, 21490])]\n",
    "dados_indicadores_transm.query(\"CD_CVM == 21490\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolando valores ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novas_linhas = pd.DataFrame({\n",
    "    'CD_CVM': [21490, 21490],\n",
    "    'DENOM_CIA' : ['ALUPAR INVESTIMENTO S/A', 'ALUPAR INVESTIMENTO S/A'],\n",
    "    'DT_FIM_EXERC': ['2010-06-30', '2011-06-30'],\n",
    "    '3.01': [np.nan, np.nan],\n",
    "    '3.02': [np.nan, np.nan],\n",
    "    '3.03': [np.nan, np.nan],\n",
    "    '3.04': [np.nan, np.nan],\n",
    "    '3.05': [np.nan, np.nan],\n",
    "    '3.06': [np.nan, np.nan],\n",
    "    '3.07': [np.nan, np.nan],\n",
    "    '3.08': [np.nan, np.nan],\n",
    "    '3.09': [np.nan, np.nan],\n",
    "    '3.11': [np.nan, np.nan]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_indicadores_transm = pd.concat([dados_indicadores_transm, novas_linhas], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolar os valores nulos\n",
    "dados_indicadores_transm = dados_indicadores_transm.sort_values(by=['CD_CVM', 'DT_FIM_EXERC'])\n",
    "dados_indicadores_transm = dados_indicadores_transm.interpolate(method='linear', limit_direction='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organiza indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indicadores = {}\n",
    "for cd_cvm in dados_indicadores_transm['CD_CVM'].unique():\n",
    "    df_temp = dados_indicadores_transm.copy()\n",
    "    df_temp = df_temp[df_temp['CD_CVM'] == cd_cvm].copy()\n",
    "    \n",
    "    df_temp.drop(columns=['CD_CVM','DENOM_CIA'], inplace=True)\n",
    "    df_temp.set_index('DT_FIM_EXERC', inplace=True)\n",
    "    dict_indicadores[cd_cvm] = df_temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Interpolar NaNs em dict_indicadores_estac_norm[empresa] para cada empresa\n",
    "# for emp, df in dict_indicadores.items():\n",
    "#     before = int(df.isna().sum().sum())\n",
    "#     df2 = df.copy()\n",
    "\n",
    "#     # Garantir índice datetime para interpolação por tempo\n",
    "#     try:\n",
    "#         new_index = pd.to_datetime(df2.index, errors='raise')\n",
    "#         df2 = df2.set_index(new_index).sort_index()\n",
    "#         df2 = df2.interpolate(method='time', limit_direction='both', axis=0)\n",
    "#     except Exception:\n",
    "#         # Fallback para linear se índice não for datetime\n",
    "#         df2 = df2.sort_index().interpolate(method='linear', limit_direction='both', axis=0)\n",
    "\n",
    "#     # Opcional: se sobrar algum NaN, usar ffill/bfill nas bordas\n",
    "#     if df2.isna().any().any():\n",
    "#         df2 = df2.ffill().bfill()\n",
    "\n",
    "#     dict_indicadores[emp] = df2\n",
    "#     after = int(df2.isna().sum().sum())\n",
    "#     print(f\"{emp}: NaNs {before} -> {after}\")\n",
    "\n",
    "# # Conferir uma empresa ativa\n",
    "# dict_indicadores[empresa]\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Z-score normalization para todas as colunas de um DataFrame.\n",
    "    Retorna um novo DataFrame cujas colunas são (x - mean) / std.\n",
    "    \"\"\"\n",
    "    return (df - df.mean()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tornando variáveis exógenas estacionárias\n",
    "* Normalização é aplicada depois de tornar o sinal estacionário no caso das variáveis exógenas\n",
    "* Para as variáveis dependentes, basta normalizá-las, umas vez que a parte integrativa dos modelos irá cuidar da estacionariedade. Neste caso é necessário saber apenas o grau de diferenciação que precisa ser realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tornar_estacionaria(serie, max_diff=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Aplica diferenciações sucessivas até a série temporal ficar estacionária (ADF p < 0.05).\n",
    "    Retorna a série estacionária e o número de diferenciações aplicadas.\n",
    "    \"\"\"\n",
    "    serie_estac = serie.copy()\n",
    "    diff_count = 0\n",
    "    for _ in range(max_diff + 1):\n",
    "        adf_result = adfuller(serie_estac.dropna())\n",
    "        p_value = adf_result[1]\n",
    "        if verbose:\n",
    "            print(f\"Differencing: {diff_count}, p-value: {p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            return serie_estac, diff_count\n",
    "        serie_estac = serie_estac.diff().dropna()\n",
    "        diff_count += 1\n",
    "    if verbose:\n",
    "        print(\"Atenção: não ficou estacionária após\", max_diff, \"diferenciações.\")\n",
    "    return serie_estac, diff_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_estac = dados_indices.copy()\n",
    "for column in dados_indices.columns:\n",
    "    if column != 'data':\n",
    "        indices_estac[column], diffs = tornar_estacionaria(dados_indices[column], verbose=True)\n",
    "        print(f\"Série '{column}' tornou-se estacionária após {diffs} diferenciações.\")\n",
    "indices_estac.set_index(dados_indices['data'], inplace=True)\n",
    "indices_estac = indices_estac.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicadores_estac = {}\n",
    "for cd_cvm, df in dict_indicadores.items():\n",
    "    indicadores_estac[cd_cvm] = pd.DataFrame()\n",
    "    for column in df.columns:\n",
    "        indicadores_estac[cd_cvm][column], diffs = tornar_estacionaria(df[column], verbose=True)\n",
    "        print(f\"CD_CVM {cd_cvm} - Série '{column}' tornou-se estacionária após {diffs} diferenciações.\")\n",
    "    print(len(indicadores_estac[cd_cvm]))\n",
    "    if len(indicadores_estac[cd_cvm]) == 58:\n",
    "        indicadores_estac[cd_cvm].set_index(df.index[1:], inplace=True)\n",
    "    else:\n",
    "        indicadores_estac[cd_cvm] = indicadores_estac[cd_cvm].iloc[1:, :]\n",
    "        indicadores_estac[cd_cvm].set_index(df.index[1:], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar sazonalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def plot_com_destaques(serie, n=4, offset=0, titulo=None, label='Série'):\n",
    "    s = pd.Series(serie).reset_index(drop=True)\n",
    "    x = np.arange(len(s))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, s.values, label=label, color='steelblue', linewidth=1.5)\n",
    "\n",
    "    idx = np.arange(offset, len(s), n)  # ex.: 0,4,8,... (use offset=1 para 1,5,9,...)\n",
    "    plt.scatter(idx, s.iloc[idx].values, color='crimson', s=60, zorder=3,\n",
    "                edgecolors='white', linewidths=0.6, label=f'Destaque terceiro trimestre')\n",
    "\n",
    "    plt.title(titulo or f'{label} (Destaque terceiro trimestre)')\n",
    "    plt.xlabel('Posição')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Uso com a sua série:\n",
    "for code in [2437, 18376, 20257, 21490]:\n",
    "    plot_com_destaques(dados_indicadores.query(f\"CD_CVM == {code}\")['3.11'], n=4, offset=2, titulo=f'Empresa {code} - Lucro', label='3.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(dados_indicadores.query(f\"CD_CVM == 20257\")['3.01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlação Cruzada\n",
    "\n",
    "O foco de predição será nos indicadores:\n",
    "* 3.01 (Receitas de venda de Bens e/ou Serviços)\n",
    "* 3.02 (Custo dos Bens e/ou Serviços)\n",
    "* 3.11 (Lucro ou Prejuízo consolidado do Período)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def plot_ccf(lags, corr, indice, col, empresa=None):\n",
    "    \"\"\"\n",
    "    Plota a CCF com linhas de confiança, eixos e legendas padronizados.\n",
    "    - lags: array de lags\n",
    "    - corr: array de correlações (mesmo tamanho de lags)\n",
    "    - indice: nome da variável X\n",
    "    - indicador: nome da variável Y\n",
    "    - empresa: identificador opcional\n",
    "    - n_obs: nº de observações para IC; se None usa len(corr) como aproximação\n",
    "    - alpha: nível de significância (default 5%)\n",
    "    \"\"\"\n",
    "    fig, ax =plt.subplots(figsize=(15, 6))\n",
    "    plt.stem(lags, corr, use_line_collection=True)\n",
    "    plt.title(f'CCF entre {indice} e {col} - Empresa {empresa}')\n",
    "    confidence_interval = 1.96/np.sqrt(len(corr))\n",
    "    ax.axhline(-confidence_interval, color='red', label='5% intervalo de confiança')\n",
    "    ax.axhline(confidence_interval, color='red')\n",
    "    ax.axvline(x = 0, color = 'black', lw = 1)\n",
    "    ax.axhline(y = 0, color = 'black', lw = 1)\n",
    "    ax.axhline(y = np.max(corr), color = 'blue', lw = 1,\n",
    "            linestyle='--', label = 'Maior correlação +/-')\n",
    "    ax.axhline(y = np.min(corr), color = 'blue', lw = 1,\n",
    "            linestyle='--')\n",
    "    ax.set(ylim = [-1, 1])\n",
    "    # if y_label != x_label:\n",
    "    #     ax.set_title('Correlação Cruzada ' + x_label + ' e ' + y_label, weight='bold', fontsize = 15)\n",
    "    # if y_label == x_label:\n",
    "    #     ax.set_title('Autocorrelação ' + x_label, weight='bold', fontsize = 15)\n",
    "    ax.set_ylabel('Coeficientes de Correlação', weight='bold',\n",
    "    fontsize = 12)\n",
    "    #determina os pontos marcados no eixo X\n",
    "    plt.xticks(range(min(lags), max(lags+1), 3))\n",
    "    ax.set_xlabel('Lags', weight='bold', fontsize = 12)\n",
    "    plt.legend()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a Função de Correlação Cruzada (CCF) entre índices (VIF) e apenas os indicadores alvo\n",
    "ccf_dict = {}\n",
    "alvo_indicadores = ['3.01', '3.02', '3.11']\n",
    "\n",
    "# Número de observações para o cálculo do intervalo de confiança (aprox.)\n",
    "n_obs = len(indicadores_estac[2437])\n",
    "conf_interval = 1.96 / np.sqrt(n_obs)\n",
    "\n",
    "for empresa, df in indicadores_estac.items():\n",
    "    # Indicadores alvo disponíveis neste df\n",
    "    cols_indicadores = [c for c in alvo_indicadores if c in df.columns]\n",
    "    if not cols_indicadores:\n",
    "        continue\n",
    "\n",
    "    # # Índices aprovados pelo VIF que existem em indices_norm_estac\n",
    "    # cols_indices = [c for c in vif_finais.get(empresa, pd.Series(dtype=float)).index\n",
    "    #                 if c in indices_norm_estac.columns]\n",
    "    # if not cols_indices:\n",
    "    #     continue\n",
    "\n",
    "for indice in indices_estac.columns[1:]:\n",
    "    serie_x = indices_estac[indice].dropna()\n",
    "    for empresa, df in indicadores_estac.items():\n",
    "        for col in cols_indicadores:\n",
    "            serie_y = df[col].dropna()\n",
    "\n",
    "            # Alinhar pelas datas em comum\n",
    "            #print(f'{serie_x.shape} - {serie_y.shape}')\n",
    "\n",
    "            aligned = pd.concat([serie_x, serie_y], axis=1, join='inner').dropna()\n",
    "            if len(aligned) < 3:\n",
    "                continue\n",
    "\n",
    "            corr = ccf(aligned.iloc[:, 0], aligned.iloc[:, 1], adjusted=False)\n",
    "            lags = np.arange(len(corr))\n",
    "            ccf_dict[(empresa, indice, col)] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_dict[(2437, 'ibov', '3.01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempacotar ccf_dict em um DataFrame\n",
    "records = []\n",
    "for (empresa, indice_x, indicador), ccf_arr in ccf_dict.items():\n",
    "    for lag, corr_val in enumerate(ccf_arr):\n",
    "        records.append({\n",
    "            'Indice': indice_x,\n",
    "            'Empresa': empresa,\n",
    "            'Indicador': indicador,\n",
    "            'Lag': lag,          # statsmodels.ccf retorna lags >= 0\n",
    "            'CCF': corr_val,\n",
    "            'Abs_CCF': abs(corr_val)\n",
    "        })\n",
    "\n",
    "df_ccf = pd.DataFrame(records)\n",
    "\n",
    "# Opcional: limitar por lag máximo (ex.: até 8)\n",
    "lag_max = 8\n",
    "df_ccf = df_ccf[df_ccf['Lag'] <= lag_max]\n",
    "\n",
    "# Marcar significância pelo intervalo de confiança já calculado (conf_interval)\n",
    "df_ccf['Significativo'] = df_ccf['Abs_CCF'] > conf_interval\n",
    "\n",
    "# 1) Top N lags significativos em geral\n",
    "topN = 10\n",
    "top_lags = (df_ccf[df_ccf['Significativo']]\n",
    "            .sort_values('Abs_CCF', ascending=False)\n",
    "            .head(topN))\n",
    "display(top_lags)\n",
    "\n",
    "# 2) Ranking por par (Indice, Empresa, Indicador): melhor lag por par\n",
    "idx_max = (df_ccf[df_ccf['Significativo']]\n",
    "           .groupby(['Empresa', 'Indice', 'Indicador'])['Abs_CCF']\n",
    "           .idxmax())\n",
    "ranked_pairs = (df_ccf.loc[idx_max]\n",
    "                .sort_values('Abs_CCF', ascending=False)\n",
    "                .reset_index(drop=True))\n",
    "display(ranked_pairs.head(topN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar Top 5 por (Empresa, Indicador) com base em ranked_pairs\n",
    "top_n = 5\n",
    "alvo_indicadores = ['3.01', '3.02', '3.11']  # opcional: filtre apenas os indicadores-alvo\n",
    "\n",
    "rp = ranked_pairs.copy()\n",
    "if 'Indicador' in rp.columns and alvo_indicadores:\n",
    "    rp = rp[rp['Indicador'].isin(alvo_indicadores)]\n",
    "\n",
    "for (empresa, indicador), df_group in rp.groupby(['Empresa', 'Indicador']):\n",
    "    df_top = df_group.sort_values('Abs_CCF', ascending=False).head(top_n)\n",
    "    for _, row in df_top.iterrows():\n",
    "        indice = row['Indice']\n",
    "        key = (empresa, indice, indicador)\n",
    "        corr = ccf_dict.get(key)\n",
    "        if corr is None:\n",
    "            # chave inexistente no dicionário (pode ocorrer por nomes/filtragens)\n",
    "            continue\n",
    "        lags = np.arange(len(corr))\n",
    "        plot_ccf(lags, corr, indice=indice, col=indicador, empresa=empresa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_indicadores_transm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montando dataframes de indicadores específicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_dataframe_indicador(df, coluna_valor):\n",
    "    \"\"\"\n",
    "    Cria um dataframe pivotado onde a coluna 'DT_FIM_EXERC' é o índice\n",
    "    e cada coluna corresponde ao valor da coluna especificada (coluna_valor)\n",
    "    de cada empresa (CD_CVM).\n",
    "\n",
    "    Parâmetros:\n",
    "      df: DataFrame original com as colunas 'CD_CVM', 'DT_FIM_EXERC' e a coluna desejada.\n",
    "      coluna_valor: Nome da coluna que será utilizada para os valores (ex.: 'Resultado Bruto').\n",
    "\n",
    "    Retorna:\n",
    "      Um DataFrame pivotado.\n",
    "    \"\"\"\n",
    "    resultado = pd.DataFrame()\n",
    "    for cd_cvm in df['CD_CVM'].unique():\n",
    "        df_temp = df.loc[df['CD_CVM'] == cd_cvm, ['DT_FIM_EXERC', coluna_valor]].copy()\n",
    "        df_temp = df_temp.rename(columns={coluna_valor: cd_cvm})\n",
    "        df_temp = df_temp.set_index('DT_FIM_EXERC')\n",
    "        if resultado.empty:\n",
    "            resultado = df_temp.copy()\n",
    "        else:\n",
    "            resultado = resultado.join(df_temp, how='outer')\n",
    "\n",
    "    # Preencher valores NaN com 0\n",
    "    resultado.fillna(0, inplace=True)\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_bruto = criar_dataframe_indicador(dados_indicadores_transm, '3.01')\n",
    "lucro_consolidado = criar_dataframe_indicador(dados_indicadores_transm, '3.11')\n",
    "despesas_operacionais = criar_dataframe_indicador(dados_indicadores_transm, '3.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_indicadores_transm['CD_CVM'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste de estacionariedade KPSS\n",
    "-   **Hipótese Nula (H0)**: A série é estacionária.\n",
    "-   **Hipótese Alternativa (Ha)**: A série não é estacionária.\n",
    "\n",
    "Teste de estacionariedade ADF\n",
    "-   **Hipótese Nula (H0)**: A série não é estacionária.\n",
    "-   **Hipótese Alternativa (Ha)**: A série é estacionária.\n",
    "\n",
    "obs: utilizando dois métodos distintos para confirmar a estacionariedade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste_dados_estacionarios(series):\n",
    "    dados_estacionarios_kpss = {}\n",
    "    dados_estacionarios_adf = {}\n",
    "    for cd_cvm, serie in series.items():\n",
    "        print(cd_cvm)\n",
    "        kpss_stat, p_value, lags, critical_values = kpss(serie)\n",
    "        \n",
    "        print(f'Estatística do teste: {kpss_stat:.4f}')\n",
    "        print(f'p-valor: {p_value:.4f}')\n",
    "        print('Valores Críticos:')\n",
    "        for key, value in critical_values.items():\n",
    "            print(f'{key}: {value:.4f}')\n",
    "        print('Resultado:')\n",
    "        if p_value > 0.05:\n",
    "            #dados_estacionarios_kpss[cd_cvm] = p_value\n",
    "            dados_estacionarios_kpss[cd_cvm] = 'S'\n",
    "        else:\n",
    "            #dados_estacionarios_kpss[cd_cvm] = p_value\n",
    "            dados_estacionarios_kpss[cd_cvm] = 'N'\n",
    "\n",
    "        adf_result = adfuller(serie)\n",
    "\n",
    "        print('ADF Statistic: %f' % adf_result[0])\n",
    "        print('p-value: %f' % adf_result[1])\n",
    "        # Optionally, inspect the critical values\n",
    "        for key, value in adf_result[4].items():\n",
    "            print(f'Critical Value ({key}): {value:.4f}')\n",
    "\n",
    "        if adf_result[1] < 0.05:\n",
    "            #dados_estacionarios_adf[cd_cvm] = adf_result[1]\n",
    "            dados_estacionarios_adf[cd_cvm] = 'N'\n",
    "        else:\n",
    "            dados_estacionarios_adf[cd_cvm] = 'S'\n",
    "\n",
    "        df_dados_estacionarios_KPSS = pd.DataFrame.from_dict(dados_estacionarios_kpss, orient='index', columns=['KPSS'])\n",
    "        df_dados_estacionarios_ADF = pd.DataFrame.from_dict(dados_estacionarios_adf, orient='index', columns=['ADF'])\n",
    "        \n",
    "    return df_dados_estacionarios_KPSS.join(df_dados_estacionarios_ADF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_bruto_estacionariedade = teste_dados_estacionarios(resultado_bruto)\n",
    "lucro_consolidado_estacionariedade = teste_dados_estacionarios(lucro_consolidado)\n",
    "despesas_operacionais_estacionariedade = teste_dados_estacionarios(despesas_operacionais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazenar valor do componente de integração para cada série temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nivel_integracao(series):\n",
    "\n",
    "    integration_level = {}  # Will store the integration order for each series\n",
    "    series_corrigido = pd.DataFrame()\n",
    "\n",
    "    for col in series.columns:\n",
    "        serie = series[col]\n",
    "        \n",
    "        # Apply first-order differencing\n",
    "        diff1 = serie.diff().dropna()\n",
    "        adf_result1 = adfuller(diff1)\n",
    "        \n",
    "        if adf_result1[1] < 0.05:\n",
    "            integration_level[col] = 1  # Series became stationary after 1st diff.\n",
    "            series[col] = diff1\n",
    "            print(f'{col}: Estacionário depois da primeira diferenciação (I=1)')\n",
    "        else:\n",
    "            # Aplicar diferenciação de segunda ordem\n",
    "            diff2 = diff1.diff().dropna()\n",
    "            adf_result2 = adfuller(diff2)\n",
    "            integration_level[col] = 2  # Even after 1st diff, not stationary; use 2nd diff.\n",
    "            series[col] = diff2\n",
    "            print(f'{col}: Estaionário depois da segunda diferenciação (I=2) -- p-value: {adf_result2[1]:.4f}')\n",
    "\n",
    "    print(\"\\nIntegration levels:\", integration_level)\n",
    "    df_integration_level = pd.DataFrame.from_dict(integration_level, orient='index', columns=['Integration Level'])\n",
    "    return df_integration_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nivel_int_resultado_bruto = nivel_integracao(resultado_bruto)\n",
    "nivel_int_lucro_consolidado = nivel_integracao(lucro_consolidado)\n",
    "nivel_int_despesas_operacionais = nivel_integracao(despesas_operacionais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analise ACF e PACF para séries estacionárias - fluxo para dados estacionarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_componentes_arima(serie, nlags=10, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Analisa os componentes AR e MA de uma série temporal.\n",
    "\n",
    "    Parâmetros:\n",
    "    - serie: pandas Series ou array-like com a série temporal estacionária.\n",
    "    - nlags: número de lags para avaliar.\n",
    "    - alpha: nível de significância (default = 5%).\n",
    "\n",
    "    Retorna:\n",
    "    - Um dicionário com a conclusão sobre os componentes AR (p) e MA (q).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcular ACF e PACF com intervalos de confiança\n",
    "    acf_values, acf_interval = acf(serie, nlags=nlags, alpha=alpha)\n",
    "    # Calcular ACF e PACF com intervalos de confiança\n",
    "    pacf_values, pacf_interval= pacf(serie, nlags=nlags, alpha=alpha)\n",
    "\n",
    "    N = len(serie)\n",
    "    z = norm.ppf(1 - alpha/2)  # por exemplo, 1.96 para 95% de confiança\n",
    "    \n",
    "    # Inicializa o array dos intervalos de confiança\n",
    "    ci_acf = np.zeros((nlags+1, 2))\n",
    "    ci_pacf = np.zeros((nlags+1, 2))\n",
    "    \n",
    "    # Para o lag zero, o ACF é 1 e não calculamos intervalo (ou pode ser [0,0])\n",
    "    ci_acf[0] = [0, 0]\n",
    "    ci_pacf[0] = [0, 0]\n",
    "    \n",
    "    # Calcula os intervalos para cada lag > 0 usando a fórmula de Bartlett\n",
    "    for k in range(1, nlags+1):\n",
    "        var = (1.0 / N) * (1 + 2 * np.sum( acf_values[1:k]**2 ))\n",
    "        margin = z * np.sqrt(var)\n",
    "        ci_acf[k] = [-margin, margin]\n",
    "\n",
    "        # Calcula os intervalos para cada lag > 0 usando a fórmula de Bartlett\n",
    "    for k in range(1, nlags+1):\n",
    "        var = (1.0 / N) * (1 + 2 * np.sum( pacf_values[1:k]**2 ))\n",
    "        margin = z * np.sqrt(var)\n",
    "        ci_pacf[k] = [-margin, margin]\n",
    "        \n",
    "    # Função auxiliar para determinar o maior lag significativo\n",
    "    def obter_ordem(values, confint):\n",
    "        ordem = 0\n",
    "        for lag in range(1, len(values)):  # Ignora lag 0\n",
    "            if (values[lag] < confint[lag][0]) or (values[lag] > confint[lag][1]):\n",
    "                ordem = lag\n",
    "            else:\n",
    "                # Se o primeiro não significativo aparecer, interrompe\n",
    "                break\n",
    "        return ordem\n",
    "\n",
    "    # Determinar p e q\n",
    "    #print(\"Valores ACF:\", acf_values)\n",
    "    #print(\"Valores PACF:\", pacf_values)\n",
    "    #print(\"Intervalos de Confiança ACF:\", ci_acf)\n",
    "    #print(\"Intervalos de Confiança PACF:\", ci_pacf)\n",
    "    p = obter_ordem(pacf_values, ci_pacf)\n",
    "    q = obter_ordem(acf_values, ci_acf)\n",
    "\n",
    "    # Construir conclusão\n",
    "    conclusao = {\n",
    "        'AR (p)': p,\n",
    "        'MA (q)': q,\n",
    "        'Interpretacao': f\"O modelo sugerido é ARIMA({p}, d, {q}), onde d é definido pela análise de estacionariedade.\"\n",
    "    }\n",
    "\n",
    "    return p, q, conclusao\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montando variações de ARIMA para cada empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def componentes_arima(serie_temporal, nivel_int_serie):\n",
    "    resultados = []\n",
    "    for cd_cvm, series in serie_temporal.items():\n",
    "        p,q, retorno = analisar_componentes_arima(series)\n",
    "       # print(f'CD_CVM: {cd_cvm}')\n",
    "       # print(f'AR(p): {p}, MA(q): {q}')\n",
    "        serie_temporal_arima_dict = {}\n",
    "        serie_temporal_arima_dict['CD_CVM'] = cd_cvm\n",
    "        \n",
    "        serie_temporal_arima = pd.DataFrame()\n",
    "        if p == 0 and q == 0:\n",
    "            #print('Setando p=1 para existir pelo menos um modelo AR')\n",
    "            p = 1 \n",
    "            q = 1\n",
    "        resultados.append({\n",
    "            'CD_CVM': cd_cvm,\n",
    "            'AR(p)': p,\n",
    "            'MA(q)': q\n",
    "        })\n",
    "\n",
    "    df_componentes = pd.DataFrame(resultados).set_index('CD_CVM')\n",
    "\n",
    "    #Juntando componente integrativo com os componentes AR e MA\n",
    "    df_componentes_arima = pd.concat([nivel_int_serie, df_componentes], axis=1)\n",
    "    df_componentes_arima.columns = ['I(d)', 'AR(p)', 'MA(q)']\n",
    "    df_componentes_arima.fillna(0, inplace=True)\n",
    "    df_componentes_arima['I(d)'] = df_componentes_arima['I(d)'].astype(int)\n",
    "    print(df_componentes_arima)\n",
    "    return df_componentes_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_modelo_arima(serie_temporal, df_componentes):\n",
    "    # Dicionário para armazenar os modelos ajustados para cada CD_CVM\n",
    "    modelos_arima = {}\n",
    "\n",
    "    for cd_cvm in df_componentes.index:\n",
    "        # Split dos dados em treino/validação e teste\n",
    "        n = len(serie_temporal)\n",
    "        split_point = int(n * 0.8) \n",
    "\n",
    "        p = df_componentes.loc[cd_cvm, 'AR(p)']\n",
    "        q = df_componentes.loc[cd_cvm, 'MA(q)']\n",
    "        i = df_componentes.loc[cd_cvm, 'I(d)']\n",
    "        # Como as séries já são estacionárias, definimos d = 0\n",
    "        order = (p, i, q)\n",
    "\n",
    "        serie = serie_temporal[cd_cvm].iloc[:split_point]\n",
    "        \n",
    "        try:\n",
    "            modelo = ARIMA(serie, order=order).fit()\n",
    "            modelos_arima[cd_cvm] = modelo\n",
    "            print(f'Modelo ARIMA({p}, {i}, {q}) para CD_CVM {cd_cvm} ajustado com sucesso.')\n",
    "            print(modelo.summary())\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ajustar modelo para CD_CVM {cd_cvm}: {str(e)}')\n",
    "\n",
    "    return modelos_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adicionar_lags(df, lags=4):\n",
    "    frames = df.copy()  # copia do DataFrame original\n",
    "    for lag in range(1, lags + 1):\n",
    "        lagged = df.shift(lag).copy()\n",
    "        lagged.columns = [f\"{col}_lag{lag}\" for col in df.columns]\n",
    "        frames = pd.concat([frames, lagged], axis=1)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_bruto_comp_arima = componentes_arima(resultado_bruto, nivel_int_resultado_bruto)\n",
    "lucro_consolidado_comp_arima = componentes_arima(lucro_consolidado, nivel_int_lucro_consolidado)\n",
    "despesas_operacionais_comp_arima = componentes_arima(despesas_operacionais, nivel_int_despesas_operacionais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Normalizar as variáveis exógenas\n",
    "# resultado_bruto_normalizado = pd.DataFrame(scaler.fit_transform(resultado_bruto), columns=resultado_bruto.columns, index=resultado_bruto.index)\n",
    "# lucro_consolidado_normalizado = pd.DataFrame(scaler.fit_transform(lucro_consolidado), columns=resultado_bruto.columns, index=resultado_bruto.index)\n",
    "# despesas_operacionais_normalizado = pd.DataFrame(scaler.fit_transform(despesas_operacionais), columns=resultado_bruto.columns, index=resultado_bruto.index)\n",
    "\n",
    "# Normalizar as variáveis exógenas com z-score\n",
    "resultado_bruto_normalizado = normalize_dataframe(resultado_bruto)\n",
    "lucro_consolidado_normalizado = normalize_dataframe(lucro_consolidado)\n",
    "despesas_operacionais_normalizado = normalize_dataframe(despesas_operacionais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bruto_arima = gera_modelo_arima(resultado_bruto, resultado_bruto_comp_arima)\n",
    "lucro_consolidado_arima = gera_modelo_arima(lucro_consolidado, lucro_consolidado_comp_arima)\n",
    "despesas_operacionais_arima = gera_modelo_arima(despesas_operacionais, despesas_operacionais_comp_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_arima(serie, ordem_arima, n_splits=8):\n",
    "\n",
    "    # Configurando o TimeSeriesSplit\n",
    "    # Vamos dividir os dados em 8 janelas de teste. A janela de treino irá crescer a cada passo.\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    # Listas para armazenar os resultados de cada dobra da validação\n",
    "    valores_reais = []\n",
    "    previsoes = []\n",
    "\n",
    "    print(\"Iniciando a validação Walk-Forward...\")\n",
    "    # --- 3. O Loop de Validação Walk-Forward ---\n",
    "    # O tscv.split(serie) gera os índices de treino e teste para cada dobra\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(serie)):\n",
    "        print(f\"--- Dobra {i+1}/{n_splits} ---\")\n",
    "        \n",
    "        # 1. Separar os dados de treino e teste da dobra atual\n",
    "        treino = serie.iloc[train_index]\n",
    "        teste = serie.iloc[test_index]\n",
    "        print(f\"Tamanho do Treino: {len(treino)}, Tamanho do Teste: {len(teste)}\")\n",
    "        \n",
    "        # 2. Treinar o modelo ARIMA APENAS com os dados de treino\n",
    "        # O modelo é retreinado a cada passo com mais dados, simulando a realidade\n",
    "        modelo = sm.tsa.ARIMA(treino, order=ordem_arima, )\n",
    "        modelo_ajustado = modelo.fit()\n",
    "        \n",
    "        # 3. Fazer a previsão para o período de teste\n",
    "        # O número de passos da previsão é o tamanho do conjunto de teste\n",
    "        previsao = modelo_ajustado.forecast(steps=len(teste))\n",
    "        \n",
    "        # 4. Guardar os resultados\n",
    "        previsoes.extend(previsao)\n",
    "        valores_reais.extend(teste)\n",
    "\n",
    "    print(\"\\nValidação Walk-Forward concluída.\")\n",
    "\n",
    "    # --- 4. Avaliação e Visualização dos Resultados ---\n",
    "\n",
    "    # Calcular a métrica de erro final sobre todos os pontos previstos\n",
    "    rmse_final = np.sqrt(mean_squared_error(valores_reais, previsoes))\n",
    "    print(f\"\\nRMSE Final (out-of-sample): {rmse_final:.4f}\")\n",
    "\n",
    "    # Criar um DataFrame com os resultados para facilitar a visualização\n",
    "    df_resultados = pd.DataFrame({\n",
    "        'Real': valores_reais,\n",
    "        'Previsao': previsoes\n",
    "    }, index=serie.index[-len(valores_reais):]) # Garante que o índice de datas está correto\n",
    "\n",
    "    # Plotar os resultados\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(serie, label='Série Original Completa', color='gray')\n",
    "    plt.plot(df_resultados['Real'], label='Valores Reais (Teste)', color='blue', marker='o', linestyle='None')\n",
    "    plt.plot(df_resultados['Previsao'], label='Previsões Walk-Forward', color='red', linestyle='--')\n",
    "    plt.title('Validação Walk-Forward - Previsões vs. Real', fontsize=16)\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_bruto[empresa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empresa in resultado_bruto.keys():\n",
    "    print(f\"\\nEmpresa {empresa} - Ordem ARIMA: {resultado_bruto_comp_arima.loc[empresa].tolist()}\")\n",
    "    walk_forward_arima(resultado_bruto[empresa], ordem_arima=resultado_bruto_comp_arima.loc[empresa].tolist(), n_splits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empresa in lucro_consolidado.keys():\n",
    "    print(f\"\\nEmpresa {empresa} - Ordem ARIMA: {lucro_consolidado_comp_arima.loc[empresa].tolist()}\")\n",
    "    walk_forward_arima(lucro_consolidado[empresa], ordem_arima=lucro_consolidado_comp_arima.loc[empresa].tolist(), n_splits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for empresa in despesas_operacionais.keys():\n",
    "    print(f\"\\nEmpresa {empresa} - Ordem ARIMA: {despesas_operacionais_comp_arima.loc[empresa].tolist()}\")\n",
    "    walk_forward_arima(despesas_operacionais[empresa], ordem_arima=despesas_operacionais_comp_arima.loc[empresa].tolist(), n_splits=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 passos de previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # Biblioteca para adicionar uma barra de progresso (útil para loops longos)\n",
    "\n",
    "\n",
    "serie = resultado_bruto[21490]\n",
    "\n",
    "\n",
    "# --- 2. Definição do Modelo e Configuração do Walk-Forward com Horizonte Fixo ---\n",
    "\n",
    "ordem_arima = (2, 1, 1)\n",
    "horizonte_previsao = 2 # <-- A MUDANÇA PRINCIPAL: queremos prever 2 passos à frente\n",
    "\n",
    "# Definir o tamanho do conjunto de treino inicial\n",
    "# Vamos usar os primeiros 5 anos (20 trimestres) para o primeiro treino\n",
    "tamanho_treino_inicial = 20\n",
    "\n",
    "# Listas para armazenar os resultados\n",
    "valores_reais = []\n",
    "previsoes_h2 = []\n",
    "indices_previsao = [] # Guardar os índices de data para o plot\n",
    "\n",
    "print(f\"Iniciando a validação Walk-Forward para horizonte h={horizonte_previsao}...\")\n",
    "# --- 3. O Loop de Validação Walk-Forward Manual ---\n",
    "# Iteramos a partir do final do treino inicial até o penúltimo ponto possível\n",
    "for i in tqdm(range(tamanho_treino_inicial, len(serie) - horizonte_previsao + 1)):\n",
    "    \n",
    "    # 1. Separar os dados de treino da iteração atual\n",
    "    treino = serie.iloc[:i]\n",
    "    \n",
    "    # 2. Treinar o modelo ARIMA com os dados de treino\n",
    "    modelo = sm.tsa.ARIMA(treino, order=ordem_arima)\n",
    "    modelo_ajustado = modelo.fit()\n",
    "    \n",
    "    # 3. Fazer a previsão de 'horizonte_previsao' passos à frente\n",
    "    previsao_passos = modelo_ajustado.forecast(steps=horizonte_previsao)\n",
    "    \n",
    "    # 4. Capturar APENAS a previsão do horizonte desejado (a segunda previsão)\n",
    "    previsao_final = previsao_passos.iloc[horizonte_previsao - 1]\n",
    "    \n",
    "    # 5. Capturar o valor real correspondente\n",
    "    indice_real = i + horizonte_previsao - 1\n",
    "    valor_real = serie.iloc[indice_real]\n",
    "    \n",
    "    # 6. Guardar os resultados\n",
    "    previsoes_h2.append(previsao_final)\n",
    "    valores_reais.append(valor_real)\n",
    "    indices_previsao.append(serie.index[indice_real])\n",
    "\n",
    "print(\"\\nValidação Walk-Forward concluída.\")\n",
    "\n",
    "# --- 4. Avaliação e Visualização dos Resultados ---\n",
    "\n",
    "# Calcular a métrica de erro final\n",
    "rmse_final_h2 = np.sqrt(mean_squared_error(valores_reais, previsoes_h2))\n",
    "print(f\"\\nRMSE Final para Horizonte h=2 (out-of-sample): {rmse_final_h2:.4f}\")\n",
    "\n",
    "# Criar um DataFrame com os resultados para facilitar a visualização\n",
    "df_resultados_h2 = pd.DataFrame({\n",
    "    'Real': valores_reais,\n",
    "    'Previsao_h2': previsoes_h2\n",
    "}, index=indices_previsao)\n",
    "\n",
    "# Plotar os resultados\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(serie, label='Série Original Completa', color='gray', alpha=0.6)\n",
    "plt.plot(df_resultados_h2['Real'], label='Valores Reais (Teste, h=2)', color='blue', marker='o', linestyle='None')\n",
    "plt.plot(df_resultados_h2['Previsao_h2'], label=f'Previsões Walk-Forward (h={horizonte_previsao})', color='red', linestyle='--')\n",
    "plt.title(f'Validação Walk-Forward - Horizonte de Previsão h={horizonte_previsao}', fontsize=16)\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Valor')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorganizando indicadores - usados nos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meça o tamanho de todas as listas do dict_indicadores\n",
    "tamanhos = {k: v.shape[0] for k, v in dict_indicadores.items()}\n",
    "tamanhos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indicadores[empresa].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adicionar lags aos indicadores\n",
    "df_lags_indicadores = {}\n",
    "dict_indicadores_select = dict_indicadores.copy()\n",
    "\n",
    "for empresa in dict_indicadores_select.keys(): \n",
    "    #Removendo indicadores que serão previstos nos modelos\n",
    "    dict_indicadores_select[empresa].drop(columns=['3.01', '3.02', '3.11'], inplace=True)\n",
    "    df_lags_indicadores[empresa] = adicionar_lags(dict_indicadores_select[empresa], lags=4)\n",
    "    df_lags_indicadores[empresa] = df_lags_indicadores[empresa].iloc[8:]\n",
    "    #renomear index\n",
    "    print(df_lags_indicadores[empresa].columns)\n",
    "    #normalizar os valores\n",
    "    df_lags_indicadores[empresa] = (df_lags_indicadores[empresa] - df_lags_indicadores[empresa].mean()) / df_lags_indicadores[empresa].std()\n",
    "\n",
    "#adicionar lags às entradas\n",
    "df_lags_entradas = adicionar_lags(indices_estac, lags=8)\n",
    "#df_lags_entradas = df_lags_entradas.iloc[4:]\n",
    "#df_lags_entradas.set_index('data', inplace=True)\n",
    "\n",
    "\n",
    "exog_vars_estac_norm_lags = {}\n",
    "for empresa in dict_indicadores.keys(): \n",
    "    #Removendo lags 0 dos indicadores, uma vez que não teremos estes valores\n",
    "    dict_indicadores_select[empresa] = dict_indicadores_select[empresa].drop(columns=['3.03', '3.04', '3.05', '3.06', '3.07', '3.08', '3.09'])\n",
    "    exog_vars_estac_norm_lags[empresa] = pd.concat([dict_indicadores_select[empresa], df_lags_entradas], axis=1)\n",
    "#antes de alimentar os modelos, dependendo da saída do modelo, tirar o indicador referente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_vars_estac_norm_lags[2437].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolher as variáveis a serem utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arimax = {}\n",
    "for empresa in dict_indicadores.keys(): \n",
    "    for indicador in ['3.01', '3.02', '3.11']:\n",
    "            input_arimax[(empresa, indicador)] = exog_vars_estac_norm_lags[empresa].loc[dict_indicadores[empresa].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_vars_estac_norm_lags[empresa][['ibov', 'ibov_lag1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupando exog_vars - sem lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizar dict_indicadores_estac\n",
    "dict_indicadores_estac_norm = {}\n",
    "for empresa in dict_indicadores_estac.keys():\n",
    "    dict_indicadores_estac_norm[empresa] = (dict_indicadores_estac[empresa] - dict_indicadores_estac[empresa].mean()) / dict_indicadores_estac[empresa].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_vars_estac = {}\n",
    "for empresa in dict_indicadores.keys(): \n",
    "    exog_vars_estac[empresa] = pd.concat([dict_indicadores_estac_norm[empresa].drop(columns=['3.01', '3.02', '3.11']), indices_norm_estac], axis=1)\n",
    "#antes de alimentar os modelos, dependendo da saída do modelo, tirar o indicador referente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_vars_estac[2437].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_vif_iterativo(df, thresh=5.0, max_iter=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Calcula VIF iterativamente removendo a variável com maior VIF até que todos fiquem <= thresh.\n",
    "    Retorna:\n",
    "      - df_filtrado: DataFrame final após remoções\n",
    "      - vif_final: Series com VIF das variáveis remanescentes\n",
    "      - removidas: DataFrame com ordem de remoção e VIF no momento da remoção\n",
    "    \"\"\"\n",
    "    # Copiar e manter apenas colunas numéricas\n",
    "    df_work = df.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # Remover colunas constantes ou quase-constantes\n",
    "    const_cols = [c for c in df_work.columns if df_work[c].nunique(dropna=True) <= 1]\n",
    "    if const_cols and verbose:\n",
    "        print(f\"Removendo colunas constantes: {const_cols}\")\n",
    "    df_work.drop(columns=const_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    # Limpeza de dados: remover linhas com NaN/Inf\n",
    "    df_work = df_work.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='any')\n",
    "\n",
    "    removidas = []\n",
    "    it = 0\n",
    "    while it < max_iter and df_work.shape[1] > 1:\n",
    "        it += 1\n",
    "        X = sm.add_constant(df_work, has_constant='add')\n",
    "        vifs = []\n",
    "        for i, col in enumerate(X.columns):\n",
    "            try:\n",
    "                val = variance_inflation_factor(X.values, i)\n",
    "            except Exception:\n",
    "                val = np.inf\n",
    "            vifs.append(val)\n",
    "        vif = pd.Series(vifs, index=X.columns)\n",
    "\n",
    "        # descartar a constante\n",
    "        if 'const' in vif.index:\n",
    "            vif = vif.drop('const', errors='ignore')\n",
    "\n",
    "        # Se todas as variáveis estão abaixo do limiar, parar\n",
    "        max_v = vif.max()\n",
    "        if pd.isna(max_v) or max_v <= thresh:\n",
    "            if verbose:\n",
    "                print(\"\\nTodos os VIFs estão abaixo do limite.\")\n",
    "            break\n",
    "\n",
    "        # Remover a variável com maior VIF (inclusive se for inf)\n",
    "        col_remover = vif.idxmax()\n",
    "        if verbose:\n",
    "            print(f\"Removendo '{col_remover}' com VIF de {max_v:.2f}\")\n",
    "        removidas.append({'variavel': col_remover, 'vif': float(max_v), 'iter': it})\n",
    "        df_work = df_work.drop(columns=[col_remover])\n",
    "\n",
    "        # Se sobrar 1 coluna, não há por que continuar\n",
    "        if df_work.shape[1] <= 1:\n",
    "            break\n",
    "\n",
    "    # VIF final das variáveis remanescentes\n",
    "    if df_work.shape[1] >= 1:\n",
    "        Xf = sm.add_constant(df_work, has_constant='add')\n",
    "        vifs_final = []\n",
    "        for i, col in enumerate(Xf.columns):\n",
    "            try:\n",
    "                val = variance_inflation_factor(Xf.values, i)\n",
    "            except Exception:\n",
    "                val = np.inf\n",
    "            vifs_final.append(val)\n",
    "        vif_final = pd.Series(vifs_final, index=Xf.columns).drop('const', errors='ignore').sort_values(ascending=False)\n",
    "    else:\n",
    "        vif_final = pd.Series(dtype=float)\n",
    "\n",
    "    removidas_df = pd.DataFrame(removidas)\n",
    "    if verbose:\n",
    "        print(\"\\nDataFrame final após remoção por VIF:\")\n",
    "        display(df_work.head())\n",
    "        print(\"\\nValores finais de VIF:\")\n",
    "        display(vif_final)\n",
    "\n",
    "    return df_work, vif_final, removidas_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: aplicar em todos os dataframes de exog_vars_estac\n",
    "vif_filtrados = {}\n",
    "vif_finais = {}\n",
    "vif_remocoes = {}\n",
    "\n",
    "for empresa, df_exog in exog_vars_estac.items():\n",
    "    print(f\"\\n=== Empresa {empresa} ===\")\n",
    "    df_filtrado, vif_final, removidas = calcular_vif_iterativo(df_exog, thresh=5.0, verbose=True)\n",
    "    vif_filtrados[empresa] = df_filtrado\n",
    "    vif_finais[empresa] = vif_final\n",
    "    vif_remocoes[empresa] = removidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir deste momentos as entradas estão selecionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_filtrados[empresa].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlação Cruzada pós-VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Calcular a Função de Correlação Cruzada (CCF) entre índices (VIF) e apenas os indicadores alvo\n",
    "ccf_dict = {}\n",
    "alvo_indicadores = ['3.01', '3.02', '3.11']\n",
    "\n",
    "# Número de observações para o cálculo do intervalo de confiança (aprox.)\n",
    "n_obs = len(indices_norm_estac)\n",
    "conf_interval = 1.96 / np.sqrt(n_obs)\n",
    "\n",
    "for empresa, df in dict_indicadores_estac.items():\n",
    "    # Indicadores alvo disponíveis neste df\n",
    "    cols_indicadores = [c for c in alvo_indicadores if c in df.columns]\n",
    "    if not cols_indicadores:\n",
    "        continue\n",
    "\n",
    "    # # Índices aprovados pelo VIF que existem em indices_norm_estac\n",
    "    # cols_indices = [c for c in vif_finais.get(empresa, pd.Series(dtype=float)).index\n",
    "    #                 if c in indices_norm_estac.columns]\n",
    "    # if not cols_indices:\n",
    "    #     continue\n",
    "\n",
    "    for indice in vif_filtrados[empresa].columns:\n",
    "        serie_x = vif_filtrados[empresa][indice].dropna()\n",
    "        for col in cols_indicadores:\n",
    "            serie_y = df[col].dropna()\n",
    "\n",
    "            # Alinhar pelas datas em comum\n",
    "            aligned = pd.concat([serie_x, serie_y], axis=1, join='inner').dropna()\n",
    "            if len(aligned) < 3:\n",
    "                continue\n",
    "\n",
    "            corr = ccf(aligned.iloc[:, 0], aligned.iloc[:, 1], adjusted=False)\n",
    "            lags = np.arange(len(corr))\n",
    "            ccf_dict[(empresa, indice, col)] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def plot_ccf(lags, corr, indice, col, empresa=None):\n",
    "    \"\"\"\n",
    "    Plota a CCF com linhas de confiança, eixos e legendas padronizados.\n",
    "    - lags: array de lags\n",
    "    - corr: array de correlações (mesmo tamanho de lags)\n",
    "    - indice: nome da variável X\n",
    "    - indicador: nome da variável Y\n",
    "    - empresa: identificador opcional\n",
    "    - n_obs: nº de observações para IC; se None usa len(corr) como aproximação\n",
    "    - alpha: nível de significância (default 5%)\n",
    "    \"\"\"\n",
    "    fig, ax =plt.subplots(figsize=(15, 6))\n",
    "    plt.stem(lags, corr, use_line_collection=True)\n",
    "    plt.title(f'CCF entre {indice} e {col} - Empresa {empresa}')\n",
    "    confidence_interval = 1.96/np.sqrt(len(corr))\n",
    "    ax.axhline(-confidence_interval, color='red', label='5% intervalo de confiança')\n",
    "    ax.axhline(confidence_interval, color='red')\n",
    "    ax.axvline(x = 0, color = 'black', lw = 1)\n",
    "    ax.axhline(y = 0, color = 'black', lw = 1)\n",
    "    ax.axhline(y = np.max(corr), color = 'blue', lw = 1,\n",
    "            linestyle='--', label = 'Maior correlação +/-')\n",
    "    ax.axhline(y = np.min(corr), color = 'blue', lw = 1,\n",
    "            linestyle='--')\n",
    "    ax.set(ylim = [-1, 1])\n",
    "    # if y_label != x_label:\n",
    "    #     ax.set_title('Correlação Cruzada ' + x_label + ' e ' + y_label, weight='bold', fontsize = 15)\n",
    "    # if y_label == x_label:\n",
    "    #     ax.set_title('Autocorrelação ' + x_label, weight='bold', fontsize = 15)\n",
    "    ax.set_ylabel('Coeficientes de Correlação', weight='bold',\n",
    "    fontsize = 12)\n",
    "    #determina os pontos marcados no eixo X\n",
    "    plt.xticks(range(min(lags), max(lags+1), 3))\n",
    "    ax.set_xlabel('Lags', weight='bold', fontsize = 12)\n",
    "    plt.legend()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "plot_ccf(lags, corr, indice, col, empresa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar maiores correlações cruzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desempacotar ccf_dict em um DataFrame\n",
    "records = []\n",
    "for (empresa, indice_x, indicador), ccf_arr in ccf_dict.items():\n",
    "    for lag, corr_val in enumerate(ccf_arr):\n",
    "        records.append({\n",
    "            'Indice': indice_x,\n",
    "            'Empresa': empresa,\n",
    "            'Indicador': indicador,\n",
    "            'Lag': lag,          # statsmodels.ccf retorna lags >= 0\n",
    "            'CCF': corr_val,\n",
    "            'Abs_CCF': abs(corr_val)\n",
    "        })\n",
    "\n",
    "df_ccf = pd.DataFrame(records)\n",
    "\n",
    "# Opcional: limitar por lag máximo (ex.: até 8)\n",
    "lag_max = 8\n",
    "df_ccf = df_ccf[df_ccf['Lag'] <= lag_max]\n",
    "\n",
    "# Marcar significância pelo intervalo de confiança já calculado (conf_interval)\n",
    "df_ccf['Significativo'] = df_ccf['Abs_CCF'] > conf_interval\n",
    "\n",
    "# 1) Top N lags significativos em geral\n",
    "topN = 10\n",
    "top_lags = (df_ccf[df_ccf['Significativo']]\n",
    "            .sort_values('Abs_CCF', ascending=False)\n",
    "            .head(topN))\n",
    "display(top_lags)\n",
    "\n",
    "# 2) Ranking por par (Indice, Empresa, Indicador): melhor lag por par\n",
    "idx_max = (df_ccf[df_ccf['Significativo']]\n",
    "           .groupby(['Empresa', 'Indice', 'Indicador'])['Abs_CCF']\n",
    "           .idxmax())\n",
    "ranked_pairs = (df_ccf.loc[idx_max]\n",
    "                .sort_values('Abs_CCF', ascending=False)\n",
    "                .reset_index(drop=True))\n",
    "display(ranked_pairs.head(topN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monta o ranking das correlações (elimina o lag zero dos demais indicadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Desempacotar ccf_dict em um DataFrame\n",
    "records = []\n",
    "for (empresa, indice_x, indicador), ccf_arr in ccf_dict.items():\n",
    "    for lag, corr_val in enumerate(ccf_arr):\n",
    "        records.append({\n",
    "            'Indice': indice_x,\n",
    "            'Empresa': empresa,\n",
    "            'Indicador': indicador,\n",
    "            'Lag': lag,          # statsmodels.ccf retorna lags >= 0\n",
    "            'CCF': corr_val,\n",
    "            'Abs_CCF': abs(corr_val)\n",
    "        })\n",
    "\n",
    "df_ccf = pd.DataFrame(records)\n",
    "\n",
    "# Limitar por lag máximo (ex.: até 8)\n",
    "lag_max = 4\n",
    "df_ccf = df_ccf[df_ccf['Lag'] <= lag_max].copy()\n",
    "\n",
    "# Marcar significância\n",
    "df_ccf['Significativo'] = df_ccf['Abs_CCF'] > conf_interval\n",
    "\n",
    "# Remover apenas Lag==0 dos indicadores 3.0X (3.01..3.09)\n",
    "mask_30x_lag0 = (df_ccf['Lag'] == 0) & df_ccf['Indice'].astype(str).str.match(r'^3\\.0\\d$')\n",
    "df_ccf_filt = df_ccf[~mask_30x_lag0].copy()\n",
    "\n",
    "# Top N lags significativos (pós-filtro)\n",
    "topN = 5\n",
    "top_lags = (df_ccf_filt[df_ccf_filt['Significativo']]\n",
    "            .sort_values('Abs_CCF', ascending=False)\n",
    "            .head(topN))\n",
    "display(top_lags)\n",
    "\n",
    "# Melhor lag por par (pós-filtro)\n",
    "subset_sig = df_ccf_filt[df_ccf_filt['Significativo']]\n",
    "if not subset_sig.empty:\n",
    "    idx_max = (subset_sig\n",
    "               .groupby(['Empresa', 'Indice', 'Indicador'])['Abs_CCF']\n",
    "               .idxmax())\n",
    "    ranked_pairs = (subset_sig.loc[idx_max]\n",
    "                    .sort_values('Abs_CCF', ascending=False)\n",
    "                    .reset_index(drop=True))\n",
    "else:\n",
    "    ranked_pairs = pd.DataFrame(columns=['Empresa','Indice','Indicador','Lag','CCF','Abs_CCF','Significativo'])\n",
    "\n",
    "display(ranked_pairs.head(topN))\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_pairs.to_csv('ranked_ccf_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_pairs.query('Empresa == 2437 and Indicador == \"3.11\"').sort_values(by='Abs_CCF', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar Top 5 por (Empresa, Indicador) com base em ranked_pairs\n",
    "top_n = 5\n",
    "alvo_indicadores = ['3.01', '3.02', '3.11']  # opcional: filtre apenas os indicadores-alvo\n",
    "\n",
    "rp = ranked_pairs.copy()\n",
    "if 'Indicador' in rp.columns and alvo_indicadores:\n",
    "    rp = rp[rp['Indicador'].isin(alvo_indicadores)]\n",
    "\n",
    "for (empresa, indicador), df_group in rp.groupby(['Empresa', 'Indicador']):\n",
    "    df_top = df_group.sort_values('Abs_CCF', ascending=False).head(top_n)\n",
    "    for _, row in df_top.iterrows():\n",
    "        indice = row['Indice']\n",
    "        key = (empresa, indice, indicador)\n",
    "        corr = ccf_dict.get(key)\n",
    "        if corr is None:\n",
    "            # chave inexistente no dicionário (pode ocorrer por nomes/filtragens)\n",
    "            continue\n",
    "        lags = np.arange(len(corr))\n",
    "        plot_ccf(lags, corr, indice=indice, col=indicador, empresa=empresa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montando grupo de variáveis exógenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Monta DataFrames filtrados de exog_vars_estac_norm_lags usando ranked_pairs (indicador + lag)\n",
    "# Agora armazena por empresa e por indicador: exog_vars_lags_filtrado[empresa][indicador] = DataFrame\n",
    "\n",
    "exog_vars_lags_filtrado = {}\n",
    "\n",
    "for emp in ranked_pairs['Empresa'].unique():\n",
    "    exog_vars_lags_filtrado[emp] = {}\n",
    "    df_emp = ranked_pairs[ranked_pairs['Empresa'] == emp]\n",
    "    for indicador in df_emp['Indicador'].unique():\n",
    "        df_ind = df_emp[df_emp['Indicador'] == indicador]\n",
    "        # Para cada linha, monta o nome da coluna: indicador_lagN\n",
    "        col_names = [f\"{row['Indice']}_lag{row['Lag']}\" for _, row in df_ind.iterrows()]\n",
    "        # Filtra apenas colunas existentes no DataFrame de lags\n",
    "        cols_exist = [c for c in col_names if c in exog_vars_estac_norm_lags[emp].columns]\n",
    "        # Monta o DataFrame filtrado para este indicador\n",
    "        exog_vars_lags_filtrado[emp][indicador] = exog_vars_estac_norm_lags[emp][cols_exist].copy()\n",
    "        print(f\"Empresa {emp} - Indicador {indicador}: {len(cols_exist)} colunas -> {cols_exist}\")\n",
    "\n",
    "# Exemplo de visualização\n",
    "# Para empresa 2437 e indicador '3.11':\n",
    "display(exog_vars_lags_filtrado[2437]['3.11'].head())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "top_n = 5\n",
    "alvo_indicadores = ['3.01', '3.02', '3.11']  # opcional\n",
    "\n",
    "# Garantir que estamos aplicando a regra do Lag==0 para 3.0X também aqui\n",
    "rp = ranked_pairs.copy()\n",
    "if 'Indicador' in rp.columns and alvo_indicadores:\n",
    "    rp = rp[rp['Indicador'].isin(alvo_indicadores)]\n",
    "\n",
    "mask_lag0_30x = (rp['Lag'] == 0) & rp['Indicador'].astype(str).str.match(r'^3\\.0\\d{2}$')\n",
    "rp = rp[~mask_lag0_30x]\n",
    "\n",
    "# Top N por (Empresa, Indicador)\n",
    "rp_top = (rp.sort_values('Abs_CCF', ascending=False)\n",
    "            .groupby(['Empresa', 'Indicador'])\n",
    "            .head(top_n))\n",
    "\n",
    "# Lista de índices por empresa\n",
    "indices_por_empresa = (rp_top.groupby('Empresa')['Indice']\n",
    "                        .agg(lambda s: sorted(pd.unique(s)))\n",
    "                        .to_dict())\n",
    "\n",
    "# Dict de DataFrames filtrados por empresa (mantém apenas colunas existentes)\n",
    "indices_norm_estac_filtrado = {}\n",
    "for emp, cols in indices_por_empresa.items():\n",
    "    cols_exist = [c for c in cols if c in indices_norm_estac.columns]\n",
    "    indices_norm_estac_filtrado[emp] = indices_norm_estac[cols_exist].copy()\n",
    "    print(f'Empresa {emp}: {len(cols_exist)} colunas -> {cols_exist}')\n",
    "\n",
    "# DataFrame único com a união das colunas selecionadas em todas as empresas\n",
    "cols_union = sorted({c for cols in indices_por_empresa.values() for c in cols if c in indices_norm_estac.columns})\n",
    "indices_norm_estac_union = indices_norm_estac[cols_union].copy()\n",
    "\n",
    "# Visualização rápida\n",
    "display(indices_norm_estac_union.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste modelo Arimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_modelo_arimax(serie_temporal, df_componentes, exog_dict):\n",
    "    modelos_arimax = {}\n",
    "\n",
    "    for cd_cvm in df_componentes.index:\n",
    "        series = serie_temporal.copy()[cd_cvm][4:]\n",
    "        n = len(series)\n",
    "        split_point = int(n * 0.8)  # 80% dos dados para treino\n",
    "        print(f\"CD_CVM: {cd_cvm}, Length: {n}, Split Point: {split_point}\")\n",
    "        p = df_componentes.loc[cd_cvm, 'AR(p)']\n",
    "        q = df_componentes.loc[cd_cvm, 'MA(q)']\n",
    "        i = df_componentes.loc[cd_cvm, 'I(d)']\n",
    "        order = (p, i, q)\n",
    "\n",
    "        serie = series.iloc[:split_point]\n",
    "\n",
    "        exog_vars = exog_dict[cd_cvm].iloc[:split_point]\n",
    "\n",
    "        try:\n",
    "            print(f\"Serie: {serie.shape}, Exog: {exog_vars.shape}, Order: {order}\")\n",
    "            modelo = ARIMA(serie, order=order, exog=exog_vars[:split_point]).fit()\n",
    "            modelos_arimax[cd_cvm] = modelo\n",
    "            #print(f'Modelo ARIMAX({p}, {i}, {q}) para CD_CVM {cd_cvm} ajustado com sucesso.')\n",
    "            #print(modelo.summary())\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao ajustar modelo ARIMAX para CD_CVM {cd_cvm}: {str(e)}')\n",
    "\n",
    "    return modelos_arimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_bruto_norm = {}\n",
    "for empresa in resultado_bruto.columns:\n",
    "    resultado_bruto_norm[empresa] = (resultado_bruto[empresa] - resultado_bruto[empresa].mean()) / resultado_bruto[empresa].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepara input para o modelo ARIMAX\n",
    "for empresa in exog_vars_estac.keys():\n",
    "    #remover colunas com NaN\n",
    "    \n",
    "    zeros_por_coluna = (exog_vars_estac[empresa] == 0).sum()\n",
    "    display(zeros_por_coluna[zeros_por_coluna > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {}\n",
    "for empresa in exog_vars_estac.keys():\n",
    "    input[empresa] = exog_vars_estac[empresa]['incc'][8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {}\n",
    "for empresa in exog_vars_estac.keys():\n",
    "    input[empresa] = exog_vars_lags_filtrado[emp][indicador].iloc[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input[2437]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_arimax_resultado_bruto = gera_modelo_arimax(resultado_bruto_norm, resultado_bruto_comp_arima, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arimax_forecast_result = aplica_modelo_forecast(modelo_arimax_resultado_bruto, resultado_bruto_normalizado,input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_result_final['ARIMAX RMSE'] = arimax_forecast_result['RMSE']\n",
    "df_combined_result_final['ARIMAX MAE'] = arimax_forecast_result['MAE']\n",
    "df_combined_result_final['ARIMAX RMSE Improvement (%)'] = ((df_combined_result_final['Naive RMSE'] - df_combined_result_final['ARIMAX RMSE']) / df_combined_result_final['Naive RMSE']) * 100\n",
    "df_combined_result_final.sort_values(by='ARIMAX RMSE Improvement (%)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista de dicionários com os valores de AIC e BIC para cada cd_cvm -> RESULTADO BRUTO\n",
    "performance = []\n",
    "for cd_cvm, modelo in result_bruto_arima.items():\n",
    "    performance.append({\n",
    "        'CD_CVM': cd_cvm,\n",
    "        'AIC': modelo.aic,\n",
    "        'BIC': modelo.bic\n",
    "    })\n",
    "\n",
    "# Cria o DataFrame e define CD_CVM como índice\n",
    "df_performance_resul_bruto = pd.DataFrame(performance).set_index('CD_CVM')\n",
    "\n",
    "# Ordena o DataFrame pelo AIC (menor é melhor)\n",
    "df_performance_resul_bruto = df_performance_resul_bruto.sort_values(by='AIC')\n",
    "df_combined_result = df_performance_resul_bruto.join(arimax_forecast_result)\n",
    "print(df_combined_result.sort_values(by='CD_CVM'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
